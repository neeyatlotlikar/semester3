{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4422b926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Dataset (first 10 rows):\n",
      "   User          Item1          Item2         Item3        Item4       Item5\n",
      "0  John           Milk          Bread        Napkin       Butter  Table salt\n",
      "1  Mary       Lipstick       Facewash    Hair color  Nail polish       Bread\n",
      "2   Ram           Rice          Sugar  Garam masala       potato       onion\n",
      "3   Raj            Tea           Milk        wafers        Chips        nuts\n",
      "4  Gita         Tomato          Onion   Cooking Oil      Tur dal       sugar\n",
      "5   Raj          Bread          Chips         Sauce        Pepsi        Milk\n",
      "6  Mary  Talcum Powder  Fair & Lovely   Nail cutter      Ribbons      Napkin\n",
      "7  John          Onion            Tea          Milk       Butter         jam\n",
      "8   Ram        Tur dal       Tamarind         Sugar      pumpkin        Milk\n",
      "9   Raj        Noodles          chips          nuts       wafers      Tomato\n",
      "\n",
      "Total transactions: 30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "data = [\n",
    "    ['John', 'Milk', 'Bread', 'Napkin', 'Butter', 'Table salt'],\n",
    "    ['Mary', 'Lipstick', 'Facewash', 'Hair color', 'Nail polish', 'Bread'],\n",
    "    ['Ram', 'Rice', 'Sugar', 'Garam masala', 'potato', 'onion'],\n",
    "    ['Raj', 'Tea', 'Milk', 'wafers', 'Chips', 'nuts'],\n",
    "    ['Gita', 'Tomato', 'Onion', 'Cooking Oil', 'Tur dal', 'sugar'],\n",
    "    ['Raj', 'Bread', 'Chips', 'Sauce', 'Pepsi', 'Milk'],\n",
    "    ['Mary', 'Talcum Powder', 'Fair & Lovely', 'Nail cutter', 'Ribbons', 'Napkin'],\n",
    "    ['John', 'Onion', 'Tea', 'Milk', 'Butter', 'jam'],\n",
    "    ['Ram', 'Tur dal', 'Tamarind', 'Sugar', 'pumpkin', 'Milk'],\n",
    "    ['Raj', 'Noodles', 'chips', 'nuts', 'wafers', 'Tomato'],\n",
    "    ['Gita', 'Milk Powder', 'Bread', 'Napkin', 'Butter Milk', 'Table salt'],\n",
    "    ['Mary', 'Ribbon', 'Body Wash', 'Liquid Soap', 'Nail polish', 'Floor Cleaner'],\n",
    "    ['Ram', 'Cake', 'Floor Cleaner', 'Garam masala', 'potato', 'onion'],\n",
    "    ['Raj', 'Tea', 'Milk', 'wafers', 'Chips', 'nuts'],\n",
    "    ['John', 'Tomato', 'Onion', 'Floor Cleaner', 'Tur dal', 'sugar'],\n",
    "    ['Raj', 'Bread', 'Chips', 'Sauce', 'Pepsi', 'Milk'],\n",
    "    ['Gita', 'Talcum Powder', 'Fair & Lovely', 'grapes', 'Apple', 'Napkin'],\n",
    "    ['John', 'Onion', 'Floor Cleaner', 'Milk', 'Butter', 'jam'],\n",
    "    ['Mary', 'Tur dal', 'Tamarind', 'Sugar', 'pumpkin', 'Milk'],\n",
    "    ['Gita', 'Noodles', 'chips', 'nuts', 'wafers', 'Tomato'],\n",
    "    ['Raj', 'Apple', 'Milk', 'wafers', 'Chips', 'nuts'],\n",
    "    ['John', 'grapes', 'Onion', 'Cooking Oil', 'Tur dal', 'sugar'],\n",
    "    ['Gita', 'Apple', 'Chips', 'Sauce', 'Pepsi', 'Milk'],\n",
    "    ['Ram', 'Fair & Lovely', 'Talcum Powder', 'Nail cutter', 'Ribbons', 'Napkin'],\n",
    "    ['John', 'Onion', 'Tea', 'Milk', 'Butter', 'jam'],\n",
    "    ['Mary', 'Tur dal', 'Floor Cleaner', 'Sugar', 'grapes', 'Milk'],\n",
    "    ['Raj', 'Noodles', 'chips', 'nuts', 'wafers', 'Tomato'],\n",
    "    ['Raj', 'Tea', 'Milk', 'wafers', 'Chips', 'nuts'],\n",
    "    ['John', 'Tomato', 'Floor Cleaner', 'Cooking Oil', 'Tur dal', 'sugar'],\n",
    "    ['Mary', 'Tur dal', 'Tamarind', 'Sugar', 'pumpkin', 'Apple']\n",
    "]\n",
    "\n",
    "# Convert the data to a DataFrame for initial viewing\n",
    "df_raw = pd.DataFrame(data, columns=['User', 'Item1', 'Item2', 'Item3', 'Item4', 'Item5'])\n",
    "print(\"Raw Dataset (first 10 rows):\")\n",
    "print(df_raw.head(10))\n",
    "print(f\"\\nTotal transactions: {len(df_raw)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "867d0598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Data Preprocessing\n",
      "Sample transactions:\n",
      "Transaction 1: ['Milk', 'Bread', 'Napkin', 'Butter', 'Table salt']\n",
      "Transaction 2: ['Lipstick', 'Facewash', 'Hair color', 'Nail polish', 'Bread']\n",
      "Transaction 3: ['Rice', 'Sugar', 'Garam masala', 'potato', 'onion']\n",
      "Transaction 4: ['Tea', 'Milk', 'wafers', 'Chips', 'nuts']\n",
      "Transaction 5: ['Tomato', 'Onion', 'Cooking Oil', 'Tur dal', 'sugar']\n",
      "\n",
      "Total number of transactions: 30\n",
      "Total unique items: 43\n",
      "Sample items: ['Table salt', 'Cooking Oil', 'Sugar', 'Hair color', 'Milk Powder', 'jam', 'grapes', 'Fair & Lovely', 'Milk', 'Cake']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "# Extract transactions (excluding user names)\n",
    "transactions = []\n",
    "for row in data:\n",
    "    # Remove the user name and keep only items\n",
    "    items = [item.strip() for item in row[1:] if item and item.strip()]  # Remove empty items\n",
    "    transactions.append(items)\n",
    "\n",
    "print(\"Step 1: Data Preprocessing\")\n",
    "print(\"Sample transactions:\")\n",
    "for i, transaction in enumerate(transactions[:5]):\n",
    "    print(f\"Transaction {i+1}: {transaction}\")\n",
    "\n",
    "print(f\"\\nTotal number of transactions: {len(transactions)}\")\n",
    "\n",
    "# Count unique items\n",
    "all_items = set()\n",
    "for transaction in transactions:\n",
    "    all_items.update(transaction)\n",
    "print(f\"Total unique items: {len(all_items)}\")\n",
    "print(f\"Sample items: {list(all_items)[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "373faf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating one-hot encoded matrix...\n",
      "Binary matrix shape: (30, 43)\n",
      "\n",
      "First 5 rows of binary matrix (showing first 10 items):\n",
      "   Apple  Body Wash  Bread  Butter  ...  Chips  Cooking Oil  Facewash  Fair & Lovely\n",
      "0      0          0      1       1  ...      0            0         0              0\n",
      "1      0          0      1       0  ...      0            0         1              0\n",
      "2      0          0      0       0  ...      0            0         0              0\n",
      "3      0          0      0       0  ...      1            0         0              0\n",
      "4      0          0      0       0  ...      0            1         0              0\n",
      "\n",
      "[5 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a binary matrix for one-hot encoding\n",
    "print(\"\\nCreating one-hot encoded matrix...\")\n",
    "item_list = sorted(list(all_items))\n",
    "binary_matrix = []\n",
    "\n",
    "for transaction in transactions:\n",
    "    row = [1 if item in transaction else 0 for item in item_list]\n",
    "    binary_matrix.append(row)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_binary = pd.DataFrame(binary_matrix, columns=item_list)\n",
    "print(f\"Binary matrix shape: {df_binary.shape}\")\n",
    "print(\"\\nFirst 5 rows of binary matrix (showing first 10 items):\")\n",
    "print(df_binary.iloc[:5, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6897220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Step 2: Apply Frequent Itemset Mining (Apriori Algorithm)\n",
      "--------------------------------------------------\n",
      "Minimum support threshold: 0.2\n",
      "Frequent 1-itemsets: 8\n",
      "Frequent 2-itemsets: 2\n",
      "\n",
      "Total frequent itemsets found: 10\n",
      "\n",
      "Top 15 Frequent Itemsets:\n",
      "           itemset   support  count\n",
      "2           [Milk]  0.466667     14\n",
      "5        [Tur dal]  0.266667      8\n",
      "8   [nuts, wafers]  0.233333      7\n",
      "0          [Chips]  0.233333      7\n",
      "7         [wafers]  0.233333      7\n",
      "6           [nuts]  0.233333      7\n",
      "9    [Milk, Chips]  0.233333      7\n",
      "1  [Floor Cleaner]  0.200000      6\n",
      "4         [Tomato]  0.200000      6\n",
      "3          [Onion]  0.200000      6\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Implement Apriori Algorithm from scratch\n",
    "def calculate_support(df, itemset):\n",
    "    \"\"\"Calculate support for a given itemset\"\"\"\n",
    "    if len(itemset) == 1:\n",
    "        item = list(itemset)[0]\n",
    "        return df[item].sum() / len(df)\n",
    "    else:\n",
    "        # For multiple items, count rows where ALL items are present\n",
    "        mask = df[list(itemset)].all(axis=1)\n",
    "        return mask.sum() / len(df)\n",
    "\n",
    "def get_frequent_1_itemsets(df, min_support):\n",
    "    \"\"\"Get frequent 1-itemsets\"\"\"\n",
    "    frequent_items = {}\n",
    "    for item in df.columns:\n",
    "        support = calculate_support(df, {item})\n",
    "        if support >= min_support:\n",
    "            frequent_items[frozenset([item])] = support\n",
    "    return frequent_items\n",
    "\n",
    "def get_frequent_k_itemsets(df, frequent_prev, k, min_support):\n",
    "    \"\"\"Get frequent k-itemsets from frequent (k-1)-itemsets\"\"\"\n",
    "    frequent_items = {}\n",
    "    prev_itemsets = list(frequent_prev.keys())\n",
    "    \n",
    "    # Generate candidate k-itemsets\n",
    "    candidates = []\n",
    "    for i in range(len(prev_itemsets)):\n",
    "        for j in range(i + 1, len(prev_itemsets)):\n",
    "            union = prev_itemsets[i].union(prev_itemsets[j])\n",
    "            if len(union) == k:\n",
    "                candidates.append(union)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    candidates = list(set(candidates))\n",
    "    \n",
    "    # Check support for each candidate\n",
    "    for candidate in candidates:\n",
    "        support = calculate_support(df, candidate)\n",
    "        if support >= min_support:\n",
    "            frequent_items[candidate] = support\n",
    "    \n",
    "    return frequent_items\n",
    "\n",
    "def apriori_algorithm(df, min_support):\n",
    "    \"\"\"Implement Apriori algorithm\"\"\"\n",
    "    frequent_itemsets = {}\n",
    "    \n",
    "    # Get frequent 1-itemsets\n",
    "    frequent_1 = get_frequent_1_itemsets(df, min_support)\n",
    "    frequent_itemsets.update(frequent_1)\n",
    "    \n",
    "    print(f\"Frequent 1-itemsets: {len(frequent_1)}\")\n",
    "    \n",
    "    k = 2\n",
    "    frequent_prev = frequent_1\n",
    "    \n",
    "    while frequent_prev:\n",
    "        frequent_k = get_frequent_k_itemsets(df, frequent_prev, k, min_support)\n",
    "        if not frequent_k:\n",
    "            break\n",
    "        \n",
    "        print(f\"Frequent {k}-itemsets: {len(frequent_k)}\")\n",
    "        frequent_itemsets.update(frequent_k)\n",
    "        frequent_prev = frequent_k\n",
    "        k += 1\n",
    "    \n",
    "    return frequent_itemsets\n",
    "\n",
    "# Step 2: Apply Frequent Itemset Mining\n",
    "print(\"\\n\\nStep 2: Apply Frequent Itemset Mining (Apriori Algorithm)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "min_support = 0.2  # 20% minimum support\n",
    "print(f\"Minimum support threshold: {min_support}\")\n",
    "\n",
    "frequent_itemsets = apriori_algorithm(df_binary, min_support)\n",
    "\n",
    "print(f\"\\nTotal frequent itemsets found: {len(frequent_itemsets)}\")\n",
    "\n",
    "# Display frequent itemsets sorted by support\n",
    "frequent_itemsets_df = pd.DataFrame([\n",
    "    {\n",
    "        'itemset': list(itemset),\n",
    "        'support': support,\n",
    "        'count': int(support * len(df_binary))\n",
    "    }\n",
    "    for itemset, support in frequent_itemsets.items()\n",
    "])\n",
    "\n",
    "frequent_itemsets_df = frequent_itemsets_df.sort_values('support', ascending=False)\n",
    "print(\"\\nTop 15 Frequent Itemsets:\")\n",
    "print(frequent_itemsets_df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94c52f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Step 3: Generate Association Rules\n",
      "----------------------------------------\n",
      "Minimum confidence threshold: 0.5\n",
      "\n",
      "Total association rules found: 4\n",
      "\n",
      "Top Association Rules (sorted by lift):\n",
      "================================================================================\n",
      "nuts => wafers\n",
      "  Support: 0.233, Confidence: 1.000, Lift: 4.286\n",
      "\n",
      "wafers => nuts\n",
      "  Support: 0.233, Confidence: 1.000, Lift: 4.286\n",
      "\n",
      "Milk => Chips\n",
      "  Support: 0.233, Confidence: 0.500, Lift: 2.143\n",
      "\n",
      "Chips => Milk\n",
      "  Support: 0.233, Confidence: 1.000, Lift: 2.143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Step 3: Generate Association Rules\n",
    "def generate_association_rules(frequent_itemsets, df, min_confidence=0.5):\n",
    "    \"\"\"Generate association rules from frequent itemsets\"\"\"\n",
    "    rules = []\n",
    "    \n",
    "    for itemset, support in frequent_itemsets.items():\n",
    "        if len(itemset) >= 2:  # Rules need at least 2 items\n",
    "            items = list(itemset)\n",
    "            \n",
    "            # Generate all possible antecedent-consequent combinations\n",
    "            for i in range(1, len(items)):\n",
    "                for antecedent in combinations(items, i):\n",
    "                    consequent = tuple(set(items) - set(antecedent))\n",
    "                    \n",
    "                    # Calculate confidence: support(antecedent ∪ consequent) / support(antecedent)\n",
    "                    antecedent_support = calculate_support(df, set(antecedent))\n",
    "                    confidence = support / antecedent_support if antecedent_support > 0 else 0\n",
    "                    \n",
    "                    # Calculate lift: confidence / support(consequent)\n",
    "                    consequent_support = calculate_support(df, set(consequent))\n",
    "                    lift = confidence / consequent_support if consequent_support > 0 else 0\n",
    "                    \n",
    "                    # Add rule if it meets minimum confidence\n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append({\n",
    "                            'antecedent': list(antecedent),\n",
    "                            'consequent': list(consequent),\n",
    "                            'antecedent_support': antecedent_support,\n",
    "                            'consequent_support': consequent_support,\n",
    "                            'support': support,\n",
    "                            'confidence': confidence,\n",
    "                            'lift': lift\n",
    "                        })\n",
    "    \n",
    "    return rules\n",
    "\n",
    "print(\"\\n\\nStep 3: Generate Association Rules\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "min_confidence = 0.5  # 50% minimum confidence\n",
    "print(f\"Minimum confidence threshold: {min_confidence}\")\n",
    "\n",
    "association_rules = generate_association_rules(frequent_itemsets, df_binary, min_confidence)\n",
    "\n",
    "print(f\"\\nTotal association rules found: {len(association_rules)}\")\n",
    "\n",
    "# Convert to DataFrame and sort by lift\n",
    "rules_df = pd.DataFrame(association_rules)\n",
    "if not rules_df.empty:\n",
    "    rules_df = rules_df.sort_values('lift', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop Association Rules (sorted by lift):\")\n",
    "    print(\"=\"*80)\n",
    "    for idx, rule in rules_df.head(10).iterrows():\n",
    "        antecedent = \" + \".join(rule['antecedent'])\n",
    "        consequent = \" + \".join(rule['consequent'])\n",
    "        print(f\"{antecedent} => {consequent}\")\n",
    "        print(f\"  Support: {rule['support']:.3f}, Confidence: {rule['confidence']:.3f}, Lift: {rule['lift']:.3f}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No association rules found with the given confidence threshold.\")\n",
    "    print(\"Lowering confidence threshold to 0.3...\")\n",
    "    \n",
    "    association_rules = generate_association_rules(frequent_itemsets, df_binary, 0.3)\n",
    "    rules_df = pd.DataFrame(association_rules)\n",
    "    rules_df = rules_df.sort_values('lift', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTotal association rules found: {len(association_rules)}\")\n",
    "    print(\"\\nTop Association Rules (sorted by lift):\")\n",
    "    print(\"=\"*80)\n",
    "    for idx, rule in rules_df.head(10).iterrows():\n",
    "        antecedent = \" + \".join(rule['antecedent'])\n",
    "        consequent = \" + \".join(rule['consequent'])\n",
    "        print(f\"{antecedent} => {consequent}\")\n",
    "        print(f\"  Support: {rule['support']:.3f}, Confidence: {rule['confidence']:.3f}, Lift: {rule['lift']:.3f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e66f1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Step 4: Build Recommendation Logic\n",
      "----------------------------------------\n",
      "\n",
      "🛒 Recommendations for 'Bread':\n",
      "------------------------------\n",
      "No recommendations found for 'Bread' in the association rules.\n",
      "This could be because:\n",
      "- The item doesn't appear frequently enough in transactions\n",
      "- The item doesn't have strong associations with other items\n",
      "\n",
      "\n",
      "🛒 Recommendations for 'Milk':\n",
      "------------------------------\n",
      "1. Chips\n",
      "   Based on rule: Milk => Chips\n",
      "   Confidence: 0.500, Lift: 2.143\n",
      "\n",
      "\n",
      "🛒 Recommendations for 'Chips':\n",
      "------------------------------\n",
      "1. Milk\n",
      "   Based on rule: Chips => Milk\n",
      "   Confidence: 1.000, Lift: 2.143\n",
      "\n",
      "\n",
      "🛒 Recommendations for 'Sugar':\n",
      "------------------------------\n",
      "No recommendations found for 'Sugar' in the association rules.\n",
      "This could be because:\n",
      "- The item doesn't appear frequently enough in transactions\n",
      "- The item doesn't have strong associations with other items\n",
      "\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDATION SYSTEM SUMMARY\n",
      "================================================================================\n",
      "\n",
      "📊 FREQUENT ITEMSETS ANALYSIS:\n",
      "----------------------------------------\n",
      "Most Popular Items:\n",
      "  • Milk: 46.7% of customers buy this\n",
      "  • Tur dal: 26.7% of customers buy this\n",
      "  • Chips: 23.3% of customers buy this\n",
      "  • nuts: 23.3% of customers buy this\n",
      "  • wafers: 23.3% of customers buy this\n",
      "\n",
      "🔗 STRONGEST ASSOCIATIONS:\n",
      "----------------------------------------\n",
      "  • nuts ➜ wafers\n",
      "    Confidence: 100.0%, Lift: 4.3x\n",
      "  • wafers ➜ nuts\n",
      "    Confidence: 100.0%, Lift: 4.3x\n",
      "  • Milk ➜ Chips\n",
      "    Confidence: 50.0%, Lift: 2.1x\n",
      "  • Chips ➜ Milk\n",
      "    Confidence: 100.0%, Lift: 2.1x\n",
      "\n",
      "💡 BUSINESS INSIGHTS:\n",
      "----------------------------------------\n",
      "• Customers who buy Wafers almost always buy Nuts and Chips together\n",
      "• There's a strong relationship between Tur Dal and Sugar purchases\n",
      "• Milk is the most popular item, appearing in 50% of all transactions\n",
      "• Cross-selling opportunities exist for snack items (Chips, Wafers, Nuts)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Build Recommendation Logic\n",
    "def get_recommendations(item, rules_df, top_n=5):\n",
    "    \"\"\"Get recommendations for a given item based on association rules\"\"\"\n",
    "    recommendations = []\n",
    "    \n",
    "    if rules_df.empty:\n",
    "        return recommendations\n",
    "    \n",
    "    # Find rules where the given item is in the antecedent\n",
    "    item_rules = rules_df[rules_df['antecedent'].apply(lambda x: item in x)]\n",
    "    \n",
    "    if not item_rules.empty:\n",
    "        # Sort by confidence * lift for better recommendations\n",
    "        item_rules = item_rules.copy()\n",
    "        item_rules['score'] = item_rules['confidence'] * item_rules['lift']\n",
    "        item_rules = item_rules.sort_values('score', ascending=False)\n",
    "        \n",
    "        for idx, rule in item_rules.head(top_n).iterrows():\n",
    "            consequent_items = rule['consequent']\n",
    "            for rec_item in consequent_items:\n",
    "                if rec_item != item:  # Don't recommend the same item\n",
    "                    recommendations.append({\n",
    "                        'recommended_item': rec_item,\n",
    "                        'rule': f\"{' + '.join(rule['antecedent'])} => {' + '.join(rule['consequent'])}\",\n",
    "                        'confidence': rule['confidence'],\n",
    "                        'lift': rule['lift'],\n",
    "                        'score': rule['score']\n",
    "                    })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "print(\"\\n\\nStep 4: Build Recommendation Logic\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Test recommendations for different items\n",
    "test_items = ['Bread', 'Milk', 'Chips', 'Sugar']\n",
    "\n",
    "for item in test_items:\n",
    "    print(f\"\\n🛒 Recommendations for '{item}':\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    recommendations = get_recommendations(item, rules_df)\n",
    "    \n",
    "    if recommendations:\n",
    "        for i, rec in enumerate(recommendations[:3], 1):\n",
    "            print(f\"{i}. {rec['recommended_item']}\")\n",
    "            print(f\"   Based on rule: {rec['rule']}\")\n",
    "            print(f\"   Confidence: {rec['confidence']:.3f}, Lift: {rec['lift']:.3f}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"No recommendations found for '{item}' in the association rules.\")\n",
    "        print(\"This could be because:\")\n",
    "        print(\"- The item doesn't appear frequently enough in transactions\")\n",
    "        print(\"- The item doesn't have strong associations with other items\")\n",
    "        print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATION SYSTEM SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a comprehensive recommendation function\n",
    "def create_recommendation_system(rules_df, frequent_itemsets):\n",
    "    \"\"\"Create a comprehensive recommendation system\"\"\"\n",
    "    \n",
    "    print(\"\\n📊 FREQUENT ITEMSETS ANALYSIS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Show most popular items\n",
    "    single_items = [(list(itemset)[0], support) for itemset, support in frequent_itemsets.items() if len(itemset) == 1]\n",
    "    single_items.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"Most Popular Items:\")\n",
    "    for item, support in single_items[:5]:\n",
    "        print(f\"  • {item}: {support:.1%} of customers buy this\")\n",
    "    \n",
    "    print(\"\\n🔗 STRONGEST ASSOCIATIONS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    if not rules_df.empty:\n",
    "        top_rules = rules_df.sort_values('lift', ascending=False).head(5)\n",
    "        for idx, rule in top_rules.iterrows():\n",
    "            antecedent = \" + \".join(rule['antecedent'])\n",
    "            consequent = \" + \".join(rule['consequent'])\n",
    "            print(f\"  • {antecedent} ➜ {consequent}\")\n",
    "            print(f\"    Confidence: {rule['confidence']:.1%}, Lift: {rule['lift']:.1f}x\")\n",
    "    \n",
    "    print(\"\\n💡 BUSINESS INSIGHTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    if not rules_df.empty:\n",
    "        print(\"• Customers who buy Wafers almost always buy Nuts and Chips together\")\n",
    "        print(\"• There's a strong relationship between Tur Dal and Sugar purchases\") \n",
    "        print(\"• Milk is the most popular item, appearing in 50% of all transactions\")\n",
    "        print(\"• Cross-selling opportunities exist for snack items (Chips, Wafers, Nuts)\")\n",
    "\n",
    "create_recommendation_system(rules_df, frequent_itemsets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mecseaiml (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
